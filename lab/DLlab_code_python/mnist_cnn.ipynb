{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "2ZfaQIUpDZvO"
   },
   "source": [
    "# Classification of hand-written digits\n",
    "\n",
    "Start by loading the tensorflow package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ME0y6IHDZvV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "from six.moves import urllib\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7hW6rEcLDZvk"
   },
   "source": [
    "The following functions are needed to load the data. Just execute the cell below, do not bother about all the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "MsIgyGIYDZvo",
    "outputId": "9bfe83a4-241a-4d5b-9eec-84871b76bf9e"
   },
   "outputs": [],
   "source": [
    "SOURCE_URL = 'https://storage.googleapis.com/cvdf-datasets/mnist/'\n",
    "WORK_DIRECTORY = 'data'\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "NUM_LABELS = 10\n",
    "def maybe_download(filename):\n",
    "  \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n",
    "  if not tf.gfile.Exists(WORK_DIRECTORY):\n",
    "    tf.gfile.MakeDirs(WORK_DIRECTORY)\n",
    "  filepath = os.path.join(WORK_DIRECTORY, filename)\n",
    "  if not tf.gfile.Exists(filepath):\n",
    "    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n",
    "    with tf.gfile.GFile(filepath) as f:\n",
    "      size = f.size()\n",
    "    print('Successfully downloaded', filename, size, 'bytes.')\n",
    "  return filepath\n",
    "\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "  \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "  Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "  \"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    bytestream.read(16)\n",
    "    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n",
    "    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "  \"\"\"Extract the labels into a vector of int64 label IDs.\"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    bytestream.read(8)\n",
    "    buf = bytestream.read(1 * num_images)    \n",
    "    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)    \n",
    "    # one hot encode\n",
    "    onehot_encoded = list()\n",
    "    for value in labels:\n",
    "        letter = [0 for _ in range(0,NUM_LABELS)]\n",
    "        letter[value] = 1\n",
    "        onehot_encoded.append(letter)    \n",
    "    labels_onehot_encoded  = np.array(onehot_encoded)\n",
    "    return labels_onehot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download, extract and load the mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Get the data.\n",
    "train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n",
    "train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n",
    "test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n",
    "test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "# Extract it into numpy arrays.\n",
    "train_data = extract_data(train_data_filename, 60000)\n",
    "train_labels = extract_labels(train_labels_filename, 60000)\n",
    "test_data = extract_data(test_data_filename, 10000)\n",
    "test_labels = extract_labels(test_labels_filename, 10000)\n",
    "\n",
    "# Dataset size\n",
    "train_size = train_labels.shape[0]\n",
    "test_size = test_labels.shape[0]\n",
    "\n",
    "# Normalize the data\n",
    "train_data = train_data/255.0\n",
    "test_data = test_data/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JetLsyAjDZv3"
   },
   "source": [
    "### The model\n",
    "\n",
    "Define placeholders for the data. The input data Xim are $28\\times 28$ pixels grayscale images. The first dimension None will be the number of data points, determined when launching the graph. This placeholder is flattend into a matrix with $28\\times 28=784$ columns, where each colum represents one pixel. The placeholder for the output data Y is a one hot encoding representing the 10 different classes. One hot encoding for the class \"0\" is [1 0 0 0 0 0 0 0 0 0], for class \"1\" is [0 1 0 0 0 0 0 0 0 0] and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dY4YhlKADZv8"
   },
   "outputs": [],
   "source": [
    "# The placeholder for the input\n",
    "Xim = tf.placeholder(dtype = tf.float32, shape = [None, 28, 28, 1])\n",
    "\n",
    "# Number of data points (in the present batch)\n",
    "batchsize = tf.shape(Xim)[[0]]\n",
    "\n",
    "# Flatten the data into a matrix with  28 x 28 = 784 columns\n",
    "X = tf.reshape(tensor = Xim, shape = [batchsize,784 ])\n",
    "\n",
    "# The placeholder of the output. \n",
    "Y = tf.placeholder(dtype = tf.float32, shape = [None, 10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9EIqz-8xDZwD"
   },
   "source": [
    "Define the weights of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "17o1UGjoDZwF"
   },
   "outputs": [],
   "source": [
    "input_data = 784\n",
    "M1 = 4 #number of channels\n",
    "M2 = 8\n",
    "M3 = 12\n",
    "M4 = 200 #number of hidden units\n",
    "classes = 10\n",
    "\n",
    "W1 = tf.Variable(initial_value = tf.truncated_normal([5,5,1,M1], stddev = 0.1))\n",
    "b1 = tf.Variable(initial_value = tf.ones(M1)/M1)\n",
    "\n",
    "W2 = tf.Variable(initial_value = tf.truncated_normal([5,5,M1,M2], stddev = 0.1))\n",
    "b2 = tf.Variable(initial_value = tf.ones(M2)/M2)\n",
    "\n",
    "W3 = tf.Variable(initial_value = tf.truncated_normal([4,4,M2,M3], stddev = 0.1))\n",
    "b3 = tf.Variable(initial_value = tf.ones(M3)/M3)\n",
    "\n",
    "W4 = tf.Variable(initial_value = tf.truncated_normal([7*7*M3, M4], stddev = 0.1))\n",
    "b4 = tf.Variable(initial_value = tf.ones(M4)/M4)\n",
    "\n",
    "W5 = tf.Variable(initial_value = tf.truncated_normal([M4, classes], stddev = 0.1))\n",
    "b5 = tf.Variable(initial_value = tf.ones(classes)/classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCkPZjz-DZwM"
   },
   "source": [
    "Define the actual network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7wXZv2X7DZwP"
   },
   "outputs": [],
   "source": [
    "# The network model\n",
    "stride1 = 1\n",
    "stride2 = 2\n",
    "H1 = tf.nn.relu(tf.nn.conv2d(Xim,W1,strides=[1,stride1,stride1,1], padding='SAME') + b1)\n",
    "H2 = tf.nn.relu(tf.nn.conv2d(H1,W2,strides=[1,stride2,stride2,1], padding='SAME') + b2)\n",
    "H3 = tf.nn.relu(tf.nn.conv2d(H2,W3,strides=[1,stride2,stride2,1], padding='SAME') + b3)\n",
    "H3flat = tf.reshape(tensor = H3, shape = [batchsize, 7*7*M3])\n",
    "H4 = tf.nn.relu(tf.matmul(H3flat, W4) + b4)\n",
    "Z = tf.matmul(H4, W5) + b5\n",
    "P = tf.nn.softmax(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NYMZmV_GDZwa"
   },
   "source": [
    "### The training \n",
    "Define the loss function and the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cOpD7LKcDZwe"
   },
   "outputs": [],
   "source": [
    "# The loss function: Compute the cross-entropy\n",
    "#cross_entropy = -tf.reduce_mean(tf.reduce_sum(Y * tf.log(P), reduction_indices=1))\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = Z, labels = Y)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Define the training\n",
    "gamma = 0.002 # The learning rate\n",
    "train_step = tf.train.AdamOptimizer(gamma).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vSpeO0byDZwx"
   },
   "source": [
    "Accuracy of the trained model. 0 is worst and 1 is the best.\n",
    "These lines are for evaluating the performance. Don't bother to much about them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1B5xLOnDZw0"
   },
   "outputs": [],
   "source": [
    "prediction = tf.argmax(P, 1) # In each row of Q, determine the index of the column with highest probability\n",
    "true = tf.argmax(Y, 1) \n",
    "correct_prediction = tf.equal(true, prediction) # Compare with true labels. TRUE if correct, FALSE if not.\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "rJ4jX9FDDZxB"
   },
   "source": [
    "Launch the graph and train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "colab_type": "code",
    "id": "pid_KnP5DZxD",
    "outputId": "837ddb2a-fa37-4ac7-dd67-e0e6043732cf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train acc. 0.15  train cross-entropy 2.26 test acc. 0.089  test cross-entropy 2.323\n",
      "Step 100: train acc. 0.94  train cross-entropy 0.19 test acc. 0.929  test cross-entropy 0.246\n",
      "Step 200: train acc. 0.98  train cross-entropy 0.09 test acc. 0.954  test cross-entropy 0.148\n",
      "Step 300: train acc. 0.96  train cross-entropy 0.24 test acc. 0.959  test cross-entropy 0.124\n",
      "Step 400: train acc. 0.98  train cross-entropy 0.08 test acc. 0.967  test cross-entropy 0.099\n",
      "Step 500: train acc. 0.97  train cross-entropy 0.08 test acc. 0.973  test cross-entropy 0.089\n",
      "Step 600: train acc. 0.97  train cross-entropy 0.08 test acc. 0.976  test cross-entropy 0.075\n",
      "Step 700: train acc. 0.97  train cross-entropy 0.07 test acc. 0.975  test cross-entropy 0.079\n",
      "Step 800: train acc. 0.99  train cross-entropy 0.06 test acc. 0.978  test cross-entropy 0.076\n",
      "Step 900: train acc. 0.99  train cross-entropy 0.05 test acc. 0.980  test cross-entropy 0.062\n",
      "Step 1000: train acc. 0.99  train cross-entropy 0.03 test acc. 0.981  test cross-entropy 0.056\n",
      "Step 1100: train acc. 1.00  train cross-entropy 0.02 test acc. 0.980  test cross-entropy 0.064\n",
      "Step 1200: train acc. 0.99  train cross-entropy 0.02 test acc. 0.979  test cross-entropy 0.060\n",
      "Step 1300: train acc. 0.99  train cross-entropy 0.03 test acc. 0.981  test cross-entropy 0.061\n",
      "Step 1400: train acc. 0.97  train cross-entropy 0.08 test acc. 0.980  test cross-entropy 0.061\n",
      "Step 1500: train acc. 1.00  train cross-entropy 0.03 test acc. 0.979  test cross-entropy 0.067\n",
      "Step 1600: train acc. 0.98  train cross-entropy 0.07 test acc. 0.983  test cross-entropy 0.049\n",
      "Step 1700: train acc. 1.00  train cross-entropy 0.03 test acc. 0.984  test cross-entropy 0.050\n",
      "Step 1800: train acc. 1.00  train cross-entropy 0.01 test acc. 0.979  test cross-entropy 0.064\n",
      "Step 1900: train acc. 0.99  train cross-entropy 0.02 test acc. 0.984  test cross-entropy 0.049\n",
      "Step 2000: train acc. 0.99  train cross-entropy 0.03 test acc. 0.985  test cross-entropy 0.046\n",
      "Step 2100: train acc. 0.98  train cross-entropy 0.05 test acc. 0.985  test cross-entropy 0.044\n",
      "Step 2200: train acc. 1.00  train cross-entropy 0.01 test acc. 0.984  test cross-entropy 0.048\n",
      "Step 2300: train acc. 0.99  train cross-entropy 0.03 test acc. 0.983  test cross-entropy 0.047\n",
      "Step 2400: train acc. 1.00  train cross-entropy 0.01 test acc. 0.986  test cross-entropy 0.043\n",
      "Step 2500: train acc. 1.00  train cross-entropy 0.01 test acc. 0.985  test cross-entropy 0.049\n",
      "Step 2600: train acc. 1.00  train cross-entropy 0.02 test acc. 0.987  test cross-entropy 0.045\n",
      "Step 2700: train acc. 1.00  train cross-entropy 0.01 test acc. 0.984  test cross-entropy 0.051\n",
      "Step 2800: train acc. 0.99  train cross-entropy 0.02 test acc. 0.984  test cross-entropy 0.052\n",
      "Step 2900: train acc. 1.00  train cross-entropy 0.02 test acc. 0.985  test cross-entropy 0.052\n",
      "Step 3000: train acc. 0.99  train cross-entropy 0.03 test acc. 0.987  test cross-entropy 0.043\n",
      "Step 3100: train acc. 1.00  train cross-entropy 0.00 test acc. 0.987  test cross-entropy 0.043\n",
      "Step 3200: train acc. 1.00  train cross-entropy 0.01 test acc. 0.985  test cross-entropy 0.048\n",
      "Step 3300: train acc. 1.00  train cross-entropy 0.00 test acc. 0.985  test cross-entropy 0.044\n",
      "Step 3400: train acc. 1.00  train cross-entropy 0.01 test acc. 0.984  test cross-entropy 0.050\n",
      "Step 3500: train acc. 1.00  train cross-entropy 0.01 test acc. 0.986  test cross-entropy 0.047\n",
      "Step 3600: train acc. 1.00  train cross-entropy 0.01 test acc. 0.984  test cross-entropy 0.046\n",
      "Step 3700: train acc. 1.00  train cross-entropy 0.01 test acc. 0.987  test cross-entropy 0.041\n",
      "Step 3800: train acc. 0.99  train cross-entropy 0.01 test acc. 0.985  test cross-entropy 0.048\n",
      "Step 3900: train acc. 1.00  train cross-entropy 0.01 test acc. 0.984  test cross-entropy 0.055\n",
      "Step 4000: train acc. 0.99  train cross-entropy 0.02 test acc. 0.988  test cross-entropy 0.045\n"
     ]
    }
   ],
   "source": [
    "# Create session and initialize  variables\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Initialize the test and training error resutls\n",
    "test_accuracy = []\n",
    "test_cross_entropy = []\n",
    "test_iter = []\n",
    "train_accuracy = []\n",
    "train_cross_entropy = []\n",
    "train_iter = []\n",
    "\n",
    "BATCH_SIZE = 100 # Batch size\n",
    "NUM_ITERATIONS = 4000 # Total number of iterations\n",
    "iter_in_one_epoch = np.floor_divide(train_size,BATCH_SIZE) # Number of iterations in each epoch\n",
    "# The main training loop\n",
    "for t in range(0, NUM_ITERATIONS+1):\n",
    "    \n",
    "    # The iteration number in the present epoch\n",
    "    j = (t%iter_in_one_epoch)\n",
    "    \n",
    "    # If a new epoch, reshuffle the data indices\n",
    "    if j == 0:\n",
    "        train_ind = np.random.choice(train_size,size=train_size, replace=False)        \n",
    "        \n",
    "    # Compute the indices of the batch\n",
    "    offset = j*BATCH_SIZE\n",
    "    batch_ind = train_ind[offset:(offset+BATCH_SIZE)]\n",
    "    \n",
    "    # Extract a batch with BATCH_SIZE images and labels\n",
    "    batch_xs = train_data[batch_ind, ...]\n",
    "    batch_ys = train_labels[batch_ind,]        \n",
    "    \n",
    "    # Do one gradient setp\n",
    "    sess.run(train_step,feed_dict = {Xim: batch_xs, Y: batch_ys})\n",
    "\n",
    "    # Evaluate the performance on training data\n",
    "    # These lines are for evaluating the performance. Don't bother too much about them!\n",
    "\n",
    "    if (t%10 == 0):        \n",
    "        # Evaluate on test data\n",
    "        train_tmp = sess.run([accuracy,cross_entropy], feed_dict = {Xim: batch_xs, Y: batch_ys})\n",
    "        train_accuracy.append(train_tmp[0])\n",
    "        train_cross_entropy.append(train_tmp[1])\n",
    "        train_iter.append(t)\n",
    "\n",
    "    # Evaluate the performance on training data at every 100th itertation\n",
    "    # These lines are for evaluating the performance. Don't bother too much about them!\n",
    "\n",
    "    if (t%100==0):\n",
    "        # Evaluate on test data\n",
    "        test_tmp = sess.run([accuracy,cross_entropy], feed_dict = {Xim: test_data, Y: test_labels})\n",
    "        test_accuracy.append(test_tmp[0])\n",
    "        test_cross_entropy.append(test_tmp[1])\n",
    "        test_iter.append(t)\n",
    "\n",
    "        # Also print iteration number and prediction accuracy\n",
    "        print(\"Step %d: train acc. %.2f  train cross-entropy %.2f test acc. %.3f  test cross-entropy %.3f\" % (t,train_tmp[0],train_tmp[1],test_tmp[0],test_tmp[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wY9MEi2xDZxM"
   },
   "source": [
    "### The evaluation\n",
    "\n",
    "The remaining code produces the plots needed to evaluate the training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "colab_type": "code",
    "id": "L_MSLJvoD28u",
    "outputId": "2be25bd3-0568-4590-faa2-39de35251d61"
   },
   "outputs": [],
   "source": [
    "# Do the plot of cross entropy\n",
    "plt.plot(train_iter, train_cross_entropy,'b-', label='Training data (mini-batch)')\n",
    "plt.plot(test_iter, test_cross_entropy,'r-', label='Testing data')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cross-entropy')\n",
    "plt.ylim([0,min(test_cross_entropy)*3])\n",
    "plt.title('Cross-entropy')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Do the plot of accuracy\n",
    "plt.plot(train_iter, train_accuracy,'b-', label='Training data (mini-batch)')\n",
    "plt.plot(test_iter, test_accuracy,'r-', label='Testing data')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Prediction accuracy')\n",
    "plt.ylim([max(1-(1-test_accuracy[-1])*2,0),1])\n",
    "plt.title('Prediction accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on all test data\n",
    "tmp = sess.run([prediction,P], feed_dict = {Xim: test_data})\n",
    "test_pred = tmp[0]\n",
    "test_P = tmp[1]\n",
    "test_true = sess.run(true, feed_dict = {Y: test_labels})\n",
    "\n",
    "# Close the session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "colab_type": "code",
    "id": "caWJ8PwaDZxO",
    "outputId": "1d3e738d-e278-425a-ddb7-bcda6a4565f4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract 100 random test images\n",
    "batch_test_ind = np.random.choice(test_size,100)\n",
    "batch_test_ind_wrong_first = batch_test_ind[np.argsort(test_true[batch_test_ind]==test_pred[batch_test_ind])]\n",
    "\n",
    "# Do the plot of the images\n",
    "num_rows = 10\n",
    "num_cols = 10\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(num_cols, num_rows))\n",
    "\n",
    "for i, i_im in enumerate(batch_test_ind_wrong_first, 1):\n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(test_data[i_im,:,:,0], cmap=plt.cm.binary)    \n",
    "    if test_pred[i_im]==test_true[i_im]:\n",
    "          plt.text(0, 25, test_pred[i_im], fontsize=25, color='blue')\n",
    "    else:\n",
    "          plt.text(0, 25, test_pred[i_im], fontsize=25, color='red')    \n",
    "                  \n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "mnist_one_layer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
